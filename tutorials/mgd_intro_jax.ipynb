{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15141d25",
   "metadata": {},
   "source": [
    "# Introduction to Multiplexed Gradient Descent (MGD)\n",
    "In an era of emerging specialized neuromorphic hardware, the demand for backprop-free gradient descent is more than ever. MGD is a perturbative method that offers a simple and efficient path to training arbitrary networks with respect to an objective function, so long as a gradient exists.  This notebook is a minimal example toward a practical understanding of MGD. A few things to note:\n",
    " - We utilize the JAX python package for flexible and efficient neural network operations\n",
    " - Where ever the @jax.jit is used, the function that follows will be subject to \"just in time\" compilation\n",
    " - JAX requires explicit seed (random key) definitions.  Where ever a key is defined, simply think of that as a seed.\n",
    " - This can simply be thought of as preparing (compiling) the function to be used efficiently\n",
    " - We otherwise avoid obfuscating our implementation with calls to pacakages like jax.flax are even more efficent, but opaque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acc95c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import optax\n",
    "import copy\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import src.utils.helper_functions as hf\n",
    "import src.data_loader as dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbbffb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CudaDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "from jax import config\n",
    "config.update(\"jax_default_matmul_precision\", \"float32\")\n",
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c21f35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "cache_dir=\"~/.cache/huggingface/datasets\"\n",
    "\n",
    "trainset = load_dataset(\"mnist\", split = 'train', cache_dir = cache_dir)\n",
    "testset  = load_dataset(\"mnist\", split = 'test', cache_dir = cache_dir)\n",
    "\n",
    "def preprocess(sample):\n",
    "    sample['label'] = jax.nn.one_hot(sample['label'], 10)\n",
    "    return sample\n",
    "\n",
    "trainset = trainset.map(preprocess).shuffle(seed=0)\n",
    "testset = testset.map(preprocess).shuffle(seed=0)\n",
    "\n",
    "# Make datasets output in numpy format\n",
    "trainset = trainset.with_format('numpy')\n",
    "testset  = testset.with_format('numpy')\n",
    "\n",
    "# Copy to memory\n",
    "trainset = trainset[:]\n",
    "testset  = testset[:]\n",
    "\n",
    "trainset['image'] = np.array(trainset['image'])/255\n",
    "testset['image']  = np.array(testset['image'])/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409e675",
   "metadata": {},
   "source": [
    "## To Start: Initialize (any) network\n",
    " - MGD is model-free and therefore indifferent to network topology, activation function, and node variation. \n",
    " - We here implement a simple multi-layer feedforward neural network (an MLP).\n",
    " - In the recurrent tutorial, we examine more varied topologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f652dedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfiniteLoader:\n",
    "    def __init__(self, X, y, batch_size=32, shuffle=True):\n",
    "        self.X = np.array(X)\n",
    "        self.y = np.array(y)\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(X))\n",
    "        self.ptr = 0\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.ptr + self.batch_size > len(self.X):\n",
    "            self.ptr = 0\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(self.indices)\n",
    "\n",
    "        idx = self.indices[self.ptr:self.ptr+self.batch_size]\n",
    "        self.ptr += self.batch_size\n",
    "\n",
    "        xb = jnp.array(self.X[idx])\n",
    "        yb = jnp.array(self.y[idx])\n",
    "        return xb, yb\n",
    "    \n",
    "batch_size = 64\n",
    "trainloader = InfiniteLoader(X=trainset['image'], y=trainset['label'], batch_size=batch_size, shuffle=True)\n",
    "testloader  = InfiniteLoader(X=testset['image'], y=testset['label'], batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5869b4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dense_0': {'bias': (128,), 'kernel': (784, 128)}, 'Dense_1': {'bias': (128,), 'kernel': (128, 128)}, 'Dense_2': {'bias': (10,), 'kernel': (128, 10)}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    hidden_sizes: list[int]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        for size in self.hidden_sizes[:-1]:\n",
    "            x = nn.relu(nn.Dense(size)(x))\n",
    "        x = nn.Dense(self.hidden_sizes[-1])(x)\n",
    "        return x\n",
    "    \n",
    "x_batch , y_batch = next(trainloader)\n",
    "layer_dims        = [128,128,10]\n",
    "\n",
    "model = MLP(hidden_sizes=layer_dims)\n",
    "\n",
    "key    = jax.random.PRNGKey(0)\n",
    "params = model.init(key, x_batch)['params']\n",
    "print(jax.tree.map(lambda p: p.shape, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5228d45",
   "metadata": {},
   "source": [
    "## 1. Make a (totally normal) foward pass\n",
    "- The forward pass for MGD is exactly like any old MLP forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f9423b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations of final layer (for random sample of batch):\n",
      "[-0.16950952  0.13096733  0.10776845  0.18692796  0.03496152 -0.00521973\n",
      " -0.0693294  -0.22302842  0.121273   -0.107751  ]\n"
     ]
    }
   ],
   "source": [
    "logits = model.apply({'params': params}, x_batch)\n",
    "print(f\"Activations of final layer (for random sample of batch):\")\n",
    "print(logits[np.random.randint(batch_size)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0238126",
   "metadata": {},
   "source": [
    "Finally, like with any forward pass, we determine the loss with respect to our objective function.  Here, we use cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f159d730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass cost: \n",
      "2.28932\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def loss_CE(logits, labels):\n",
    "    return optax.softmax_cross_entropy(logits=logits, labels=labels).mean()\n",
    "\n",
    "cost0 = loss_CE(logits, y_batch)\n",
    "print(f\"Forward pass cost: \\n{cost0:.6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5979beb",
   "metadata": {},
   "source": [
    "## 2. Perturb all learnable parameters simultaneously\n",
    " - Select some (small) value $\\epsilon$\n",
    " - Random perturb all learnable parameters by $\\pm \\epsilon$\n",
    " - Store those perturbation values for every parameter\n",
    " - Apply those perturbations to a copy of the original parametes\n",
    "   - In hardware, there would be no copying. The pert value would just be stored locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a6619e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def sample_perturbations(params,epsilon,i):\n",
    "    key = jax.random.PRNGKey(i, impl=None)\n",
    "    return jax.tree.map(\n",
    "        lambda p: jax.random.choice(key, jnp.array([-1,1])*epsilon, shape=(p.shape)), params\n",
    "    )\n",
    "\n",
    "@jax.jit\n",
    "def apply_perturbations(theta,perturbations):\n",
    "    return jax.tree.map(lambda param, pert: param+pert, theta, perturbations)\n",
    "\n",
    "epsilon = 1e-6\n",
    "\n",
    "# create a set of epsilon-sized perturbations for given the shape of parameters\n",
    "perturbations = sample_perturbations(params,epsilon,0)\n",
    "\n",
    "# apply perturbations to copy\n",
    "params_perturbed = apply_perturbations(params,perturbations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d8f41d",
   "metadata": {},
   "source": [
    "## 3. Perform forward pass (again), but now with perturbed parameters\n",
    " - You'll notice these results are not exactly equivalent to the originals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dba9ec0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbed forward pass cost: \n",
      "2.28932\n"
     ]
    }
   ],
   "source": [
    "logits_perturbed = logits = model.apply({'params': params_perturbed}, x_batch)\n",
    "cost_perturbed   = loss_CE(logits_perturbed, y_batch)\n",
    "\n",
    "print(f\"Perturbed forward pass cost: \\n{cost_perturbed:.6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b6278d",
   "metadata": {},
   "source": [
    "## 4. Collect the gradient\n",
    " - Collect the gradient at every parameter\n",
    " - Defined simple as the difference in orginal and perturbed cost, weighted by learning rate\n",
    " - A good default learning rate is $\\eta = 1/\\epsilon^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79de31c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in cost before and after perturbing network parameters: -4.76837e-07\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def init_grad(params):\n",
    "    return jax.tree.map(lambda p: jnp.zeros(shape=(p.shape)), params)\n",
    "\n",
    "@jax.jit\n",
    "def collect_grad(perts,delta_c,G):\n",
    "    return jax.tree.map(lambda G, p: G + p*delta_c, G, perts)\n",
    "\n",
    "delta_cost = cost_perturbed - cost0\n",
    "print(f\"Difference in cost before and after perturbing network parameters: {delta_cost:.6}\")\n",
    "\n",
    "gradient = init_grad(params)\n",
    "gradient = collect_grad(perturbations, delta_cost, gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1127217b",
   "metadata": {},
   "source": [
    "## 5. Update parameters\n",
    " - Simply apply this gradient to every parameter component-wise\n",
    " - Weight update by $\\tau_\\theta$ (the number of iterations for which the gradient was collected)\n",
    "   - Analogous to batch size\n",
    "   - In this case $\\tau_\\theta=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4970df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def MGD_update(params,G,eta,tau_theta):\n",
    "    return jax.tree.map(\n",
    "        lambda p, G: p - G*eta/tau_theta, params, G\n",
    "    )\n",
    "\n",
    "eta = 1/epsilon**2\n",
    "tau_theta = batch_size\n",
    "params = MGD_update(params,gradient,eta,tau_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff470c1",
   "metadata": {},
   "source": [
    "## That's it! Now just repeat\n",
    " - Everything else about learning is analogous to the backprop procedure\n",
    " - However, importantly, a sophisticated gradient calculation, weight transport, and network model are *not* needed\n",
    " - Instead, two forward passes, local short-term storage of perturbations, and one global broadcase are used\n",
    " - That's it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6e2f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(apply_fn, theta, x, y):\n",
    "    logits  = apply_fn({'params': theta}, x)\n",
    "    preds   = jnp.argmax(logits, axis=-1)\n",
    "    targets = jnp.argmax(y, axis=-1)\n",
    "    return (preds == targets).mean()\n",
    "\n",
    "def loss_fn_prejit(apply_fn,params,x,y):\n",
    "    logits = apply_fn({'params': params}, x)\n",
    "    loss = optax.softmax_cross_entropy(logits, y).mean()\n",
    "    return loss\n",
    "\n",
    "loss_fn = jax.jit(loss_fn_prejit, static_argnums=(0,))\n",
    "\n",
    "# deriveable normalization method\n",
    "def simple_eta_norm(K, epsilon):\n",
    "    return 1/(K*(epsilon)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c64eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 :: Epoch 1  ::  cost 2.31715  :: accuracy 0.0913     \r"
     ]
    }
   ],
   "source": [
    "# hyperparams\n",
    "epochs         = 25\n",
    "epsilon        = 1e-6\n",
    "layer_dims     = [128,64,10]\n",
    "learning_rate  = 1e2\n",
    "decay          = 0.1\n",
    "batch_size     = tau_theta = 128\n",
    "validation_mod = 1\n",
    "    \n",
    "trainloader = InfiniteLoader(X=trainset['image'], y=trainset['label'], batch_size=batch_size)\n",
    "testloader  = InfiniteLoader(X=testset['image'],  y=testset['label'],  batch_size=batch_size)\n",
    "\n",
    "x_batch , y_batch = next(trainloader)\n",
    "\n",
    "# initializing network\n",
    "model    = MLP(hidden_sizes=layer_dims)\n",
    "key      = jax.random.PRNGKey(0)\n",
    "theta    = model.init(key, x_batch)['params']\n",
    "apply_fn = model.apply\n",
    "\n",
    "# normalizing eta and weighting with learning rate\n",
    "num_params = jnp.size(jax.flatten_util.ravel_pytree(theta)[0])\n",
    "eta0       = simple_eta_norm(num_params,epsilon) * learning_rate\n",
    "\n",
    "# corresponding batch size to iterations per epoch / test\n",
    "batches      = trainloader.X.shape[0]//batch_size\n",
    "test_batches = testloader.X.shape[0]//batch_size\n",
    "iterations   = epochs * batches\n",
    "\n",
    "# for recording training data\n",
    "costs    = []\n",
    "accs     = []\n",
    "valpochs = []\n",
    "epoch    = 0\n",
    "epoch_costs      = []\n",
    "for i in range(iterations):\n",
    "\n",
    "    # setup for epoch\n",
    "    x_batch, y_batch = next(trainloader)\n",
    "    eta              = eta0/(1+decay*epoch)\n",
    "\n",
    "    # initialize an empty gradient with same shapes as parameters (theta)\n",
    "    gradient = init_grad(theta)\n",
    "\n",
    "    # Step 1: Forward pass\n",
    "    cost0  = loss_fn(apply_fn, theta, x_batch, y_batch)\n",
    "    \n",
    "    # Step 2: Perturb\n",
    "    perturbations   = sample_perturbations(theta,epsilon,i)\n",
    "    theta_perturbed = apply_perturbations(theta,perturbations)\n",
    "\n",
    "    # Step 3: Perturbed forward pass\n",
    "    cost_perturbed   = loss_fn(apply_fn, theta_perturbed, x_batch, y_batch)\n",
    "\n",
    "    # Step 4: Collect gradient\n",
    "    delta_cost = (cost_perturbed - cost0)*-1\n",
    "    gradient   = collect_grad(perturbations, delta_cost*eta, gradient)\n",
    "\n",
    "    # Step 5: Make update\n",
    "    theta = optax.apply_updates(theta, gradient)\n",
    "\n",
    "    epoch_costs.append(cost0)\n",
    "\n",
    "    # validate (test) only with specified frequency according to validation mod\n",
    "    if i % batches == 0: epoch+=1\n",
    "    if i % batches == 0 and epoch % validation_mod == 0:\n",
    "        \n",
    "        # testing prediction accuracy over all batches in test set\n",
    "        batch_accs = [compute_accuracy(apply_fn, theta, *next(testloader)) for _ in range(test_batches)]\n",
    "\n",
    "        # recording data\n",
    "        accs.append(np.mean(batch_accs))\n",
    "        valpochs.append(epoch)\n",
    "        costs.append(np.mean(epoch_costs))\n",
    "        epoch_costs = []\n",
    "\n",
    "        # updating user\n",
    "        print(f\"Iter {i} :: Epoch {epoch}  ::  cost {costs[-1]:.6}  :: accuracy {accs[-1]:.3}     \", \n",
    "              end='\\r')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-muted')\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(valpochs, accs)\n",
    "plt.plot(valpochs, costs/np.max(costs))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Training a Neural Net with MGD on MNIST\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
